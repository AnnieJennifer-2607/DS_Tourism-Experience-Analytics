{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7526a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "#from skopt import BayesSearchCV\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler,LabelEncoder\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79db404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tourism_df = pd.read_csv('Dataset_Tourism_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f4a34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Relevant Feature for model training include \n",
    "\n",
    "# Select Features\n",
    "selected_features = [\"VisitYear\",\"VisitMonth\",\"VisitModeName\",\"AttractionId\",\"Attraction\",\"AttractionType\",\"CountryId\",\"RegionId\"]\n",
    "# ENCODING CATEGORICAL FEATURES USING ONE-HOT ENCODING AND TARGET ENCODING  \n",
    "categorical_features = [\"VisitModeName\", \"AttractionType\"]\n",
    "\n",
    "df_selected = Tourism_df[selected_features + [\"Rating\"]].copy()\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "encoded_features = ohe.fit_transform(df_selected[categorical_features])\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=ohe.get_feature_names_out(categorical_features))\n",
    "# CONVERTING BOOLEAN COLUMNS TO INTEGER TYPE FOR COMPATIBILITY  \n",
    "bool_cols = df_selected.select_dtypes(include=[\"bool\"]).columns\n",
    "df_selected[bool_cols] = df_selected[bool_cols].astype(int)\n",
    "\n",
    "# DROPPING ORIGINAL CATEGORICAL COLUMNS AFTER ENCODING AND CONCATENATING ENCODED FEATURES  \n",
    "target_enc = TargetEncoder()\n",
    "df_selected[\"Attraction\"] = target_enc.fit_transform(df_selected[\"Attraction\"], df_selected[\"Rating\"])\n",
    "\n",
    "df_selected = df_selected.drop(columns=categorical_features)\n",
    "df_selected = pd.concat([df_selected, encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47ca2397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting Features (X) any target (Y)\n",
    "# SEPARATING FEATURES (X) AND TARGET VARIABLE (Y) FOR MODEL TRAINING  \n",
    "X = df_selected.drop(columns=[\"Rating\"])\n",
    "y = df_selected[\"Rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae58e7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLYING STANDARD SCALER TO NORMALIZE THE FEATURE MATRIX (X) FOR BETTER MODEL PERFORMANCE  \n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46bcf123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLITTING THE DATA INTO TRAINING (80%) AND TESTING (20%) SETS WITH A FIXED RANDOM STATE FOR REPRODUCIBILITY  \n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d2d031a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Decision Tree Regressor → MAE: 0.67, RMSE: 0.93\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    \"max_depth\": [5, 10, 15, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 3, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid, cv=5, scoring=\"r2\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best Model\n",
    "best_dt = grid_search.best_estimator_\n",
    "\n",
    "# ROUNDING PREDICTIONS TO THE NEAREST INTEGER AND CLIPPING VALUES TO ENSURE THEY FALL WITHIN THE VALID RATING RANGE (1 TO 5)  \n",
    "# CALCULATING MEAN ABSOLUTE ERROR (MAE) AND ROOT MEAN SQUARED ERROR (RMSE) TO EVALUATE MODEL PERFORMANCE  \n",
    "dt_pred = best_dt.predict(X_test)\n",
    "dt_pred = np.round(dt_pred).astype(int)\n",
    "dt_pred = np.clip(dt_pred, 1, 5)\n",
    "\n",
    "# Evaluation Metrics\n",
    "dt_mae = mean_absolute_error(y_test, dt_pred)\n",
    "dt_rmse = np.sqrt(mean_squared_error(y_test, dt_pred))\n",
    "\n",
    "print(f\"Optimized Decision Tree Regressor → MAE: {dt_mae:.2f}, RMSE: {dt_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b0e4e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor → MAE: 0.6719251842055545, RMSE: 0.9318857810976099\n"
     ]
    }
   ],
   "source": [
    "# PERFORMING RANDOMIZED SEARCH CROSS-VALIDATION TO FIND THE BEST HYPERPARAMETERS FOR THE RANDOM FOREST REGRESSOR  \n",
    "# SEARCHING OVER DIFFERENT VALUES FOR NUMBER OF ESTIMATORS, MAX DEPTH, MIN SAMPLES SPLIT, MIN SAMPLES LEAF, AND MAX FEATURES  \n",
    "# SELECTING THE BEST MODEL BASED ON MINIMIZING MEAN ABSOLUTE ERROR (MAE)  \n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_random = RandomizedSearchCV(rf, param_dist, n_iter=20, cv=5, scoring='neg_mean_absolute_error', random_state=42, n_jobs=-1)\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "best_rf = rf_random.best_estimator_\n",
    "\n",
    "# EVALUATING THE BEST RANDOM FOREST MODEL WITH ROUNDED AND CLIPPED PREDICTIONS  \n",
    "rf_pred = best_rf.predict(X_test)\n",
    "rf_pred = np.round(rf_pred).astype(int)\n",
    "rf_pred = np.clip(rf_pred, 1, 5) \n",
    "\n",
    "rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "\n",
    "print(f\"Random Forest Regressor → MAE: {rf_mae}, RMSE: {rf_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8f453f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized XGBoost → MAE: 0.67, RMSE: 0.93\n"
     ]
    }
   ],
   "source": [
    "# PERFORMING RANDOMIZED SEARCH TO TUNE XGBOOST HYPERPARAMETERS AND SELECT THE BEST MODEL  \n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(XGBRegressor(random_state=42), param_grid, cv=5, scoring=\"r2\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# EVALUATING THE TUNED XGBOOST MODEL USING MAE AND RMSE  \n",
    "# Predictions\n",
    "xgb_pred = best_xgb.predict(X_test)\n",
    "xgb_pred = np.round(xgb_pred).astype(int)\n",
    "xgb_pred = np.clip(xgb_pred, 1, 5)\n",
    "\n",
    "# Evaluation Metrics\n",
    "xgb_mae = mean_absolute_error(y_test, xgb_pred)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))\n",
    "\n",
    "print(f\"Optimized XGBoost → MAE: {xgb_mae:.2f}, RMSE: {xgb_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba20b699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encoded model saved\n",
      "One-Hot Encoded model saved\n",
      "Scaler has been saved\n",
      "Best XGBoost Model has been saved\n"
     ]
    }
   ],
   "source": [
    "# Saving model\n",
    "joblib.dump(target_enc,'Target Encoder Model(regression).pkl')\n",
    "print(\"Target encoded model saved\")\n",
    "\n",
    "joblib.dump(ohe,'One-Hot Endcoder Model(regression).pkl')\n",
    "print(\"One-Hot Encoded model saved\")\n",
    "\n",
    "joblib.dump(scaler,'Scaler(regression).pkl')\n",
    "print(\"Scaler has been saved\")\n",
    "\n",
    "joblib.dump(best_xgb,'XGBoost model(regression).pkl')\n",
    "print(\"Best XGBoost Model has been saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0178129d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.2\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "869df708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.2\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "print(xgboost.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
